{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhGiyUPJaZCR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "! pip install transformers==4.27.4\n",
        "\n",
        "! pip install torchtext==0.10.1\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\")\n",
        "torch.cuda.init()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, TextDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLY0H4dAa6jf",
        "outputId": "001851cd-4cb7-49ec-b7d7-444045f1c0d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_path = \"/content/gdrive/My Drive/model/chitchat_generator.pt\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', padding_side='left')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0Kvyj-0Qbe26"
      },
      "outputs": [],
      "source": [
        "test_path = '/content/gdrive/My Drive/dataset/combined_test.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BDuAGs3Gb4Qa"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GMXMW0ofbpZo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def getResponse(input_text, model,tokenizer, device):\n",
        "  input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "  if(len(input_ids)<=0):\n",
        "    print(input_text)\n",
        "    return None\n",
        "  input_ids = input_ids.to(device)\n",
        "  model = model.to(device)\n",
        "  output_ids = model.generate(input_ids,pad_token_id=tokenizer.eos_token_id, max_length=70,early_stopping=True)\n",
        "  output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "  messages = output_text.split(\"\\n\")\n",
        "  first_bot_response = None\n",
        "  for message in messages:\n",
        "    if message.startswith(\"Bot:\"):\n",
        "        first_bot_response = message.strip()\n",
        "        break\n",
        "  return first_bot_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBhx6Jj1gv9W",
        "outputId": "62ca48c4-acae-41ae-8ab9-a2abe1d34c68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: I hate you.\n"
          ]
        }
      ],
      "source": [
        "print(getResponse(\"User: Hello, why do you hate me?\",model, tokenizer, device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YryGXsmTeUnf"
      },
      "outputs": [],
      "source": [
        "with open('/content/gdrive/My Drive/dataset/combined_test.txt', 'r', encoding='utf-8') as f:\n",
        "    test_data = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4knbvpkz_7O",
        "outputId": "cab3594d-2b57-4ec6-f446-529d81b206cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36273"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RNIyc7UBecLG"
      },
      "outputs": [],
      "source": [
        "def prepare(test_data):\n",
        "  input = {}\n",
        "  response = {}\n",
        "  conv_id = 1\n",
        "  idx = 1\n",
        "  for i in range(len(test_data)):\n",
        "    input[idx]=\"\"\n",
        "    response[idx]=\"\"\n",
        "    if(test_data[i]==\"\\n\" or test_data[i]=='\\n'):\n",
        "      idx+=1\n",
        "  for data in test_data:\n",
        "    if(conv_id>=idx):\n",
        "      break\n",
        "    if(data.startswith(\"User\")):\n",
        "      input[conv_id]+= data \n",
        "    elif(data.startswith(\"Bot\")):\n",
        "      response[conv_id]+=data\n",
        "    else:\n",
        "      conv_id+=1\n",
        "  return input,response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "f28x0-q2ekBs"
      },
      "outputs": [],
      "source": [
        "input, response = prepare(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfQXtbu_emF4"
      },
      "outputs": [],
      "source": [
        "# from tqdm import tqdm\n",
        "# def getResponses(input, response, model,tokenizer, device):\n",
        "#   generated_response = {}\n",
        "#   with open(\"/content/gdrive/My Drive/dataset/generated_responses.json\", 'a') as f:\n",
        "#     for idx, text in tqdm(input.items()):\n",
        "#       value = getResponse(text, model,tokenizer, device)\n",
        "#       if(value==None):\n",
        "#         response2[idx]=\"None\"\n",
        "#       generated_response[idx] = value\n",
        "#       f.write(str(idx)+ \":\" )\n",
        "#       f.write(str(value))\n",
        "#       f.write(\"\\n\")\n",
        "#   return generated_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "lvb_zjVAmqJ9"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def getResponses(input, response, model,tokenizer, device):\n",
        "  generated_response = {}\n",
        "  for idx, text in tqdm(input.items()):\n",
        "    value = getResponse(text, model,tokenizer, device)\n",
        "    if(value==None):\n",
        "      response2[idx]=\"None\"\n",
        "    generated_response[idx] = value\n",
        "  return generated_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "g5DWgFFvU5T1"
      },
      "outputs": [],
      "source": [
        "input_demo_keys = list(input.keys())[:7000]\n",
        "input_demo = {key: input[key] for key in input_demo_keys}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VGaMPEKTU7Pk"
      },
      "outputs": [],
      "source": [
        "response_demo_keys = list(response.keys())[:7000]\n",
        "response_demo = {key: response[key] for key in response_demo_keys}\n",
        "response2=response_demo.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS1DoeK4m48l"
      },
      "outputs": [],
      "source": [
        "generated_response = getResponses(input_demo, response_demo, model,tokenizer, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "88a3Citm23JX"
      },
      "outputs": [],
      "source": [
        "for key, value in generated_response.items():\n",
        "  if(value is None):\n",
        "    generated_response[key]=\"None\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "c4TiLTNDMF5I"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# open file for writing\n",
        "with open(\"/content/gdrive/My Drive/dataset/generated_responses.json\", \"w\") as outfile:\n",
        "    # write dictionary to file in JSON format\n",
        "    json.dump(generated_response, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "O7b9mw9cnYYV"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# open file for writing\n",
        "with open(\"/content/gdrive/My Drive/dataset/modified_responses.json\", \"w\") as outfile:\n",
        "    # write dictionary to file in JSON format\n",
        "    json.dump(response2, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uNuSue5cyoXp"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6YLdyW0v4QP"
      },
      "outputs": [],
      "source": [
        "!pip install bert-score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jhZ6zUfwJs5"
      },
      "outputs": [],
      "source": [
        "from bert_score import score\n",
        "P, R, F1 = score(list(generated_response.values()), list(response2.values()), lang=\"en\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApKfBv_BxYBG",
        "outputId": "a57a8525-42fb-4bf5-eaed-5f6c964fef13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.869292676448822\n",
            "0.857213020324707\n",
            "0.8629080653190613\n"
          ]
        }
      ],
      "source": [
        "print(P.mean().item())\n",
        "print(R.mean().item())\n",
        "print(F1.mean().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMAZvfCpxru5",
        "outputId": "d287d89a-4e16-4fcd-d95d-1c014f8a2756"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1.1876857836124988e-231"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "bleu_score = corpus_bleu(list(generated_response.values()), list(response2.values()))\n",
        "bleu_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDDWQ5R20hDw"
      },
      "outputs": [],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "6ymKgsgp0aQl"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(list(generated_response.values()), list(response2.values()), avg=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooILDRtC4-v2",
        "outputId": "17fa03ef-2717-4745-f509-6b72ba4f5a51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'rouge-1': {'r': 0.212496510112691,\n",
              "  'p': 0.2587420801793697,\n",
              "  'f': 0.20327748922264302},\n",
              " 'rouge-2': {'r': 0.015346934717142175,\n",
              "  'p': 0.023703335199954014,\n",
              "  'f': 0.0159509371007501},\n",
              " 'rouge-l': {'r': 0.2081268174395909,\n",
              "  'p': 0.2510940967382122,\n",
              "  'f': 0.19811036254725495}}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.11.3 ('chatbot')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "a8e241135b7916f6d05cb78054452203e44357a042c94ab0c4fa42e4743117b9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
