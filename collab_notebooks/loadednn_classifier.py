# -*- coding: utf-8 -*-
"""LoadedNN_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dmXIeMTlciEH7iHmS8rz8hb8xF-N7h2a
"""

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
! pip install torchtext==0.10.1

import torch
device = torch.device("cuda")
torch.cuda.init()

from google.colab import drive
drive.mount('/content/gdrive')
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

# read the text file into a pandas dataframe
with open('/content/gdrive/My Drive/classifier/totalclassifierdata.txt', 'r') as f:
    lines = f.readlines()
    data = {'query': [], 'label': []}
    for line in lines:
        line = line.strip().split('\t')
        if len(line) >= 2:
            data['query'].append(line[0])
            data['label'].append(line[1])
        else:
            continue
    dataset = pd.DataFrame(data)

# extract the input queries and their corresponding labels
queries = dataset['query']
labels = dataset['label']

# tokenize the queries
tokenizer = Tokenizer()
tokenizer.fit_on_texts(queries)
vocab_size = len(tokenizer.word_index) + 1
sequences = tokenizer.texts_to_sequences(queries)
max_len = max([len(seq) for seq in sequences])
padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')

model = tf.keras.models.load_model('/content/gdrive/My Drive/classifier-NN')

# classify a new input query
new_query = "I am going to paris"
new_query_sequence = tokenizer.texts_to_sequences([new_query])
new_query_padded = pad_sequences(new_query_sequence, maxlen=max_len, padding='post')
prediction = np.argmax(model.predict(new_query_padded), axis=-1)
if prediction == 1:
    print("The input query is a topical query.")
else:
    print("The input query is a chitchat query.")